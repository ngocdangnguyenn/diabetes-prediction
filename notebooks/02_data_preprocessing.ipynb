{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26005958",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Diabetes Dataset\n",
    "### Introduction\n",
    "This notebook performs **data preprocessing** for the diabetes dataset analyzed in `02_exploratory_data_analysis.ipynb`. Based on the insights from the exploratory phase, we will clean and transform the raw dataset to make it suitable for feature engineering and predictive modeling.\n",
    "\n",
    "**Dataset:** Diabetes Dataset (Kaggle)\n",
    "\n",
    "**Objective:** Produce a clean dataset that can be directly used in the feature engineering stage.\n",
    "\n",
    "**Author:** NGUYEN Ngoc Dang Nguyen - Final-year Student in Computer Science, Aix-Marseille University\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "1. Import Libraries and Load Data\n",
    "2. Missing Values Handling\n",
    "3. Outliers Handling\n",
    "4. Train-Test Split\n",
    "5. Feature Scaling\n",
    "6. Save Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b210d",
   "metadata": {},
   "source": [
    "### 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4683094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/diabetes.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "print(f\"Dataset size: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "df_original = df.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed9db8",
   "metadata": {},
   "source": [
    "### 2. Missing Values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16129efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_cols = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n",
    "df[zero_cols] = df[zero_cols].replace(0, np.nan)\n",
    "summary = pd.DataFrame({\n",
    "    \"Missing Count\": df[zero_cols].isnull().sum(),\n",
    "    \"Median Used\": [df[col].median() for col in zero_cols]\n",
    "})\n",
    "\n",
    "for col in zero_cols:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "print(\"Missing Values Handling Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc83abd",
   "metadata": {},
   "source": [
    "### 3. Outliers Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Glucose\", \"BloodPressure\", \"BMI\", \"Age\"]\n",
    "for col in features:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "    print(f\"{col}: {outliers} outliers\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.boxplot(y=df[col], color='lightpink')\n",
    "    plt.title(f'{col} â€“ Boxplot')\n",
    "    plt.ylabel(col)\n",
    "plt.suptitle(\"Outlier Visualization via Boxplots\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.to_csv('../data/processed/cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5316d7",
   "metadata": {},
   "source": [
    "### 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Outcome')\n",
    "y = df['Outcome']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Train class distribution: {Counter(y_train)}\")\n",
    "print(f\"Test class distribution:  {Counter(y_test)}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.countplot(x=y_train, ax=axes[0], palette='Set2')\n",
    "axes[0].set_title(\"Train Class Distribution\")\n",
    "\n",
    "sns.countplot(x=y_test, ax=axes[1], palette='Set2')\n",
    "axes[1].set_title(\"Test Class Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8869946f",
   "metadata": {},
   "source": [
    "### 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"Scaling completed\")\n",
    "print(f\"Train scaled mean: {X_train_scaled.mean().round(3).values}\")\n",
    "print(f\"Train scaled std: {X_train_scaled.std().round(3).values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46bbd5",
   "metadata": {},
   "source": [
    "### 6. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler, '../data/processed/scaler.pkl')\n",
    "\n",
    "X_train_scaled.to_csv('../data/processed/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03928a34",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The preprocessing pipeline successfully handled missing values encoded as zeros, detected and visualized outliers, and prepared train/test splits with proper scaling. The StandardScaler was fitted exclusively on training data to prevent data leakage. Processed datasets and the fitted scaler were saved to `data/processed/` for use in feature engineering and modeling stages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
